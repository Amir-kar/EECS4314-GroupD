<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Assignment 1: Conceptual Architecture Report - Group D</title>
  <link rel="stylesheet" href="style.css">
  <style>
    
  </style>
</head>
<body>
  <div class="page">
    <header>
      <div>
        <h1>Assignment 1: Conceptual Architecture Report</h1>
        <div class="authors">Group D — Amir Karpov, Andrew Tissi, Andrew Vu, Dylan Smith, Mathew Vu</div>
      </div>
      <div class="meta">October 14, 2025</div>
    </header>

    <hr />

    <nav class="toc">
      <strong>Contents</strong>
      <a href="#abstract">Abstract</a>
      <a href="#introduction">Introduction</a>
      <a href="#hadoop">Hadoop at first glance</a>
      <a href="#architectural-styles">Architectural styles and Patterns</a>
      <a href="#yarn-overview">Yarn Overview</a>
      <a href="#workflow">High-Level YARN Workflow</a>
      <a href="#deep-dive">Yarn Conceptual Architecture - Deep Dive</a>
      <a href="#rm">Resource Manager</a>
      <a href="#am">Applications Master</a>
      <a href="#nm">Node Manager</a>
      <a href="#container">Container</a>
      <a href="#scheduler">Deep dive into the Scheduler</a>
      <a href="#federation">Deep dive into YARN Federation</a>
      <a href="#usecase">Yarn Use-Case: MapReduce Job on YARN</a>
      <a href="#derivation">Derivation Process & Alternatives</a>
      <a href="#lessons">Lessons learned</a>
      <a href="#references">References</a>
    </nav>

    <section id="abstract">
      <h2>Abstract</h2>
    <p>This report provides a conceptual architecture and a deep dive analysis of YARN (Yet Another Resource Negotiator), the resource management and job scheduling framework of Hadoop.</p>
    <p>The four main components of Hadoop are Hadoop Common (provides the shared utilities), MapReduce (Enables distributed and parallel processing), HDFS (fault tolerant distributed storage) and Yarn (managers cluster resources and schedules jobs). Their dependencies are structured in a way so that Hadoop Common provides the foundation of libraries and utilities that all other components rely on. The architectural styles within the system are: Layered, Master-Slave, Client-Server, Distributed Fault Tolerance, and Pipe & Filter. Yarn’s main goal is to decouple resource management from data processing which in turn allows for better cluster utilization and supports multiple processing engines. This report uses use cases and sequence diagrams to illustrate YARN’s workflows and interactions with other components, giving a better understanding of Hadoop’s architectural design and operating principles.</p>
    </section>

    <section id="introduction">
      <h2>Introduction</h2>
      <p>Within this report we provide a conceptual architecture of Apache Hadoop with a deep dive into Yarn. Our goal was to focus on YARN’s conceptual architecture and dependencies while only using Hadoop’s official documentation and other reputable online sources.</p>
      <ul>
        <li>The four main components of Apache Hadoop are The Common, HDFS, YARN, MapReduce [1].</li>
        <li>The four main components within YARN which were found throughout our research are: Resource Manager, Application Master, NodeManager, and Containers [4][8].</li>
        <li>We use close up diagrams to portray the components interactions and dependencies while using sequence diagrams to create a clear understanding of the systems workflow.</li>
      </ul>
      <p><em>Note:</em> When reading this report please refer to legends found within images: arrows = depends on, dotted line = data / optional / UI.</p>
    </section>

    <section id="hadoop">
      <h2>Hadoop at first glance</h2>
      <p>Apache Hadoop is composed of four core components (layers) with clear dependencies found within Apache’s public documentation:</p>
      <ul>
        <li><strong>MapReduce:</strong> This is Hadoop’s batch processor engine, performs distributed computation on the data stored in HDFS [9]. MapReduce is at the top layer (application layer) and it depends on all 3 lower layers.</li>
        <li><strong>YARN(Yet ANother Resource Negotiator):</strong> This is Hadoop’s resource allocator and jobs scheduler. It manages and schedules resources for running applications [8]. YARN is at the 2nd layer (resource management layer) and it depends on the 2 lower layers.</li>
        <li><strong>HDFS(Hadoop’s Distributed File Storage):</strong> This is Hadoop’s file storage system [10]. It breaks large data into blocks and stores as well as replicates the blocks across multiple nodes. HDFS is at the 3rd layer (storage layer) and it depends only on the bottom layer.</li>
        <li><strong>Hadoop Common:</strong> This is Hadoop common core that provides libraries, services and utilities to support the upper layer Hadoop modules [4]. The Common is at the bottom layer (foundation layer).</li>
      </ul>
      <img src="HadoopConceptualArchitecture.png" alt="Hadoop Conceptual Architecture">
      <h3>Interactions</h3>
      <ul>
        <li>The Common <-> HDFS, YARN, MapReduce: Common provides libraries, services, utilities that HDFS, YARN and MapReduce import and use.</li>
        <li>HDFS <-> YARN: YARN stores job-related data such as application JARS, logs and outputs in HDFS. YARN also retrieves application data and configurations from HDFS when launching jobs.</li>
        <li>HDFS <-> MapReduce: MapReduce reads input from HDFS blocks and writes output back to HDFS. HDFS provides data locality information so MapReduce tasks can be executed near the data blocks.</li>
        <li>YARN <-> MapReduce: YARN manages the containers for MapReduce tasks to run on.</li>
      </ul>
      <h3>Dependencies</h3>
      <ul>
        <li>HDFS, YARN and MapReduce depend on Common for core libraries, services and utilities [1]</li>
        <li>YARN depends on HDFS for storing and retrieving application data and configurations (JARs, logs, etc.) [11]</li>
        <li>MapReduce depends on HDFS for accessing input and output data blocks [11]</li>
        <li>MapReduce depends on YARN for resource management and tasks scheduling</li>
      </ul>
    </section>

    <section id="architectural-styles">
      <h2>Architectural styles and Patterns</h2>

      <table>
        <thead>
          <tr><th>Architectural styles</th><th>Description</th><th>Hadoop examples</th></tr>
        </thead>
        <tbody>
          <tr><td>Layered</td><td>System is organized in layers, each providing services to the layer above and using services from the layer below</td><td>Hadoop has 4 layers: 
          <ol>
            <li>Top: Application layer (MapReduce, Spark, Tez, …)</li>
            <li>Resource management layer (YARN handles resource allocation and jobs scheduling)</li>
            <li>Storage layer (HDFS handles distributed storage of files)</li>
            <li>Bottom: Core layer (Common provides utilities, libraries, etc. to upper layers)</li>
          </ol>
            
          </td></tr>
          <tr><td>Master Slave</td><td>One master coordinates multiple slaves that perform the actual work</td><td>
          <ol>
            <li>MapReduce: JobTracker (runs on master node) schedules and monitors jobs and TaskTrackers (run on slave nodes) work the tasks</li>
            <li>YARN: Resource Manager is the cluster master and Application Master is application master, giving instructions to NodeManagers (slaves) to work with containers</li>
            <li>HDFS: NameNode is the master, secondary Namenode is the master helper and DataNodes are slaves</li>
          </ol>  
          </td></tr>
          <tr><td>Client Server</td><td>Client sends a request to server, which processes and returns a response to client</td><td>Client sends a request (job submission) to Hadoop Server, Hadoop coordinates the work and sends back a response to client</td></tr>
          <tr><td>Distributed</td><td>Components are spread across multiple physical/virtual nodes that work together as a single system. This improves scalability and fault tolerance</td><td>Hadoop consists of multiple nodes where it can store data (HDFS stores file blocks on many DataNodes) and runs computations (YARN schedule tasks across NodeManagers)</td></tr>
          <tr><td>Pipe &amp; Filter</td><td>Data flows through a sequence of processing steps connected by pipes</td><td>MapReduce has Map and Reduce filters and Shuffle and Sort pipes: Data input -> Map Filter -> Shuffle Pipe -> Sort Pipe -> Reduce Filter -> Output</td></tr>
        </tbody>
      </table>
    </section>

    <section id="yarn-overview">
      <h2>Yarn Overview</h2>
      <p>YARN (Yet Another Resource Negotiator) is the component responsible for resource management and jobs scheduling in Hadoop. There are 4 key components in YARN: Resource Manager, Application Master, Node Manager and Container.</p>
      <ul>
        <li><strong>Resource Manager (RM):</strong> There is only 1 Resource Manager in YARN. It is the master daemon for allocating resources and scheduling jobs. The RM schedules and allocates containers.
</li>
        <li><strong>Application Master (AM):</strong> There is 1 Application Master per application. It is responsible for managing the full life cycle of its application.The AM requests for containers, and orchestrates the NM to launch and stop</li>
        <li><strong>Node Manager (NM):</strong> There is 1 Node Manager per node and an application is typically within containers located in multiple nodes. The NM is responsible for managing its node and the containers within itself</li>
        <li><strong>Container:</strong> There are multiple Containers in a node, each runs a task. A container is a resource bundle (CPU, Memory) where a task runs.
</li>
      </ul>
      <img src="yarnOverview.png" alt="Yarn Conceptual Architecture">
    </section>

    <h3>Interactions</h3>
    <ul>
      <li>Client -> RM: Client submits a ‘job’ to the RM. A ‘job’ is typically an instruction on what to run and the desired resources needed.
      </li>
      <li>AM <-> RM: AM requests and obtains containers’ information from RM; RM returns an allocation of resources along with a list of NM’s. 
      </li>
      <li>AM <-> NM: AM uses the allocations from the RM and orchestrates the NM’s to launch and monitor the allocated containers</li>
      <li>NM <->Container: NM launches the process in a local container and monitors it.
      </li>
      <li>NM <-> RM: NM registers itself with the RM, periodically sending heartbeats and status updates of its containers.
      </li>
    </ul>
    <h3>Dependencies</h3>
    <ul>
      <li>AM depends on RM for allocating an initial AM container for itself and for additional container allocations for the app’s tasks.
      </li>
      <li>AM depends on the NM to launch, monitor and stop allocated containers.</li>
      <li>NM depends on RM for cluster membership(registration).
      </li>
      <li>Containers depend on NM for resource limits, status updates, and start/stop rules.
      </li>
    </ul>
    <section id="workflow">
      <h2>High-Level YARN Workflow</h2>
      <img src="YarnWorkFlow.png" alt="Yarn Workflow">
      <h3>Workflow</h3>
      <ol>
        <li>Client submits a job to Resource Manager.</li>
        <li>Resource Manager accepts the job and allocates a container for Application Master of the job application. </li>
        <li>Resource Manager asks the Node Manager of the node containing the allocated container to start it.</li>
        <li>Node Manager launches the container as well as Application Master on it. Node Manager will then send back a heartbeat to the Resource Manager, reporting a change in its containers’ status.</li>
        <li>Application Master registers itself to Resource Manager. After that it requests then retrieves containers’ information from the Resource Manager.
        </li>
        <li>Application Master sends Node Manager the instructions to launch and assign tasks to those containers.
        </li>
        <li>Node Manager gets instructions from Application Master, resource from Resource Manager and launches the containers then assigns tasks to them.
        </li>
        <li>Node Manager sends a heartbeat to Resource Manager to update its containers’ status and Resource Manager sends a response back to the client.
        </li>
      </ol>
    </section>

    <section id="deep-dive">
      <h2>Yarn Conceptual Architecture - Deep Dive</h2>
      <p>Below we expand on the main components and their responsibilities.</p>
    </section>

    <section id="rm">
      <h2>Resource Manager</h2>
      <p>In Hadoop there exist multiple background Java processes called daemons, they run constantly to manage storage, resource allocation and task coordination across the distributed system, and use the master and slave architecture. The Resource Manager (RM) is a global master daemon, it is the ultimate authority that arbitrates resources, it has two major components: ApplicationsManager and Scheduler, and two minor components: the state store, and Web App Proxy [2]. The ApplicationManager is responsible for accepting job requests from the client(s), negotiating with the first container for executing the application specific ApplicationMaster, requests the allocation of resources from the scheduler, and provides the service for restarting the ApplicationManager container. </p>
      <br>
      <p>The scheduler allocates resources to the various running applications based on the resource requirement of the application. This is also based on the resource Container that has elements such as memory, cpu, disk, network etc. The State Store is a pluggable storage that will save application metadata and also the completion state of the application, since ResourceManager is YARN’s orchestrator it is potentially a single point of failure and State Store is there to store information while ResourceManager restarts. The Web Application Proxy is a proxy intended to reduce web based malicious attacks through YARN, ApplicationMaster has the responsibility to provide a web UI and to send that link to ResourceManager, the proxy will warn users of potentially malicious links and avoid risk [3].
      </p>
      
      <img src="ResourceManager.png" alt="ResourceManager">
    </section>

    <section id="am">
      <h2>Applications Master</h2>
      <p>The ApplicationMaster(AM) is the per-application daemon launched by the RM within its own container.  The AM acts as the application’s orchestrator by requesting for resource allocations from the RM and by instructing the NodeManagers to start/stop containers.</p>
      <br>
      <p>Within the AM there are two protocols which are used for communication. The AMRMClientAsync is used to communicate with the RM when submitting requests for resource allocations and status updates about the AMs app. The AMNMClientAsync is used to communicate with the NMs when sending start/stop and other container requests.  Note that both of the protocols used by the AM contain async within their names, this means that both the RM and NMs send callback messages to the AM.</p>
      <br>
      <p>Many AMs also expose a WebUI per-application. This WebUI is tunneled to the RM, and clients are then able to view per-app status’, metrics, and configs when accessing the RM’s WebUI Proxy.</p>
      <img src="AM.png" alt="ApplicationMaster">
      
    </section>

    <section id="nm">
      <h2>Node Manager</h2>
      <p>The NodeManager is the per-node daemon.  The NM launches and monitors containers within its machine at the AM’s request.  The AM passes along details about allocations the RM had previously confirmed, and the NM executes start/stop instructions to its containers.
      </p>
      <br>
      <p>	The NM does not allocate resources, nor does it orchestrate any real tasks.
      </p>
      <br>
      <p>The Heartbeat service within the NM sends the RM status updates about the NM. This service runs on NM startup and periodically when the NM is online.  Without the heartbeat service, there would be no method to keep track of whether an NM should be included within the cluster.  These heartbeat messages also give the RM a chance to tell the NM whether or not to clean up containers and to free up resources.
      </p>
      <div class="image-placeholder"><img src = "NM.png" alt="NodeManager"></div>
    </section>

    <section id="container">
      <h2>Container</h2>
      <p>A Container is a resource bundle where a task runs. A resource bundle includes everything a task may need (CPU, Memory, Network) to run to completion. A common example we can use to create an image is a ‘docker’.</p>  
      <br>
      <p>	Containers are hosted within nodes, and there are typically multiple containers per node. 
      </p>
      <br>
      <p>Containers come into the equation as early as the AM is allocated a container by the RM. From here, Containers are constantly being spun up and down via the control of its NM; which is at the mercy of its AM.</p>
      <br>
      <p>The Container only hosts the task and has no control over the task. Docker is an example of a resource bundle but a Container can use many other resource bundles.</p>
      <div class="image-placeholder"><img src = "Container.png" alt="Container"></div>
    </section>

    <section id="scheduler">
      <h2>Deep dive into the Scheduler</h2>
      <div class="image-placeholder"><img src="Scheduler.png" alt="Scheduler"></div>
      <p>The Scheduler is a scheduler, it is responsible for the allocation of resources to various running applications, it is a pure scheduler in that it does not monitor or track the status of the running applications [8]. The Scheduler is designed for better utilization of resources and performance enhancements [12] to maximise these values scheduler has a pluggable policy, this is responsible for partitioning the cluster resources among various queues and applications [8]. The current pluggable schedulers that Hadoop offers are FIFO, Capacity, and Fair schedulers [8].</p>
    </section>

    <section id="federation">
      <h2>Deep dive into YARN Federation</h2>
      <p>YARN Federation is an optional advanced feature built on top of YARN. It was introduced in Hadoop 3.0 to solve very large-scale cluster challenges. Attempting to scale a traditional YARN architecture with a single ResourceManager can run into bottlenecks. As the cluster size grows all resource management and scheduling responsibilities fall on one master node. YARN Federation solves this issue by splitting the system into multiple sub-clusters. Each of these subclusters gets its own ResourceManager and compute nodes. This means there will be many MasterNodes, one for each subcluster. This allows applications to schedule jobs and tasks across all nodes, while each subcluster automatically handles its own local scheduling.</p>
      <br>
      <p>With this design, applications can schedule jobs and tasks across all the nodes in the federated cluster, while each subcluster manages its own local scheduling independently. This architecture is much better for scalability as it enables linear growth: adding more sub-clusters will increase the overall cluster capacity without overloading any ResourceManagers.</p>
      <br>
      <p>YARN Federation improves efficiency as most jobs stay local, and it ensures fairness and load balancing because whenever a ResourceManager in a given subcluster gets overwhelmed, other ResourceManagers can take over some work and prevent any master node from becoming a bottleneck.</p>
      <div class="image-placeholder"><img src="YarnFederation.png" alt="Yarn Federation"></div>
      <p>At the core of YARN Federation is a StateStore, a local database that keeps track of all subclusters and load-balancing policies. It is used by both Routers and ResourceManagers to coordinate across subclusters. </p>
      <br>
      <p>Clients interact with the federated YARN through a central Router, which serves as the client’s entry point. When a client submits an application, the Router queries the StateSore to check which subcluster should be handling the request. The Router then forwards the request to the ResourceManager of the correct subcluster. That subcluster that starts the job and is called Home Subcluster. Other subclusters that participate in the application’s execution are called Secondary Subclusters. </p>
      <br>
      <p>To facilitate communication between the ApplicationMaster and multiple ResourceManagers across the many subclusters, YARN Federation employs the AMRMProxy. This proxy sits on top of each node and forwards ApplicationMaster’s resource requests to multiple ResourceManagers. The proxy does this by using an interceptor chain pattern - resembling a pipe-and-filter architecture. This allows the requests to be processed through a series of interceptors before reaching their respective ResourceManagers</p>
    </section>
    

    <section id="usecase">
      <h2>Yarn Use-Case: MapReduce Job on YARN</h2>
      <p>If space allows, include sequence diagrams showing client &rarr; RM &rarr; AM &rarr; NM and lifecycle of Map and Reduce tasks inside containers.</p>
    </section>

    <section id="derivation">
      <h2>Derivation Process &amp; Alternatives</h2>
      <p>We gathered information from Hadoop official documentation, GeekforGeeks, and DataCamp. Alternatives considered included Google BigQuery (Borg) and Snowflake.</p>
    </section>

    <section id="lessons">
      <h2>Lessons learned</h2>
      <p>YARN and Hadoop combine multiple architectural styles to achieve robustness, scalability, and fault tolerance. Single masters (NameNode, ResourceManager) are potential bottlenecks but can be mitigated using standby masters or federation.</p>
    </section>

    <section id="references">
      <h2>References</h2>
      <ol class="references">
        <li><a href="https://hadoop.apache.org/" target="_blank" rel="noopener">https://hadoop.apache.org/</a></li>
        <li><a href="https://www.geeksforgeeks.org/data-engineering/hadoop-daemons-and-their-features/" target="_blank" rel="noopener">GeeksforGeeks: Hadoop daemons and their features</a></li>
        <li><a href="https://hadoop.apache.org/docs/r3.3.1/hadoop-yarn/hadoop-yarn-site/WebApplicationProxy.html" target="_blank" rel="noopener">Hadoop WebApplicationProxy docs</a></li>
        <li><a href="https://www.datacamp.com/blog/hadoop-architecture" target="_blank" rel="noopener">DataCamp: Hadoop architecture</a></li>
        <li><a href="https://www.geeksforgeeks.org/data-engineering/hadoop-architecture/" target="_blank" rel="noopener">GeeksforGeeks: Hadoop architecture</a></li>
        <li><a href="https://www.geeksforgeeks.org/big-data/hadoop-yarn-architecture/" target="_blank" rel="noopener">GeeksforGeeks: YARN architecture</a></li>
      </ol>
    </section>

    <footer>
      <div>Group D — Assignment 1. Generated on October 14, 2025.</div>
    </footer>
  </div>
</body>
</html>
